{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cameliaguerraoui/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "nltk.download('stopwords')\n",
    "en_stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "_PATH = \"8-9_Data\"\n",
    "_FILE_NAME = \"spam_or_ham.csv\"\n",
    "_LABEL = \"label\"\n",
    "_MSG = \"messages\"\n",
    "_TOKEN = \"token\"\n",
    "_LENGTH = \"length\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Detector\n",
    "\n",
    "https://blog.paperspace.com/nlp-spam-detection-application-with-scikitlearn-xgboost/\n",
    "\n",
    "file:///Users/cameliaguerraoui/Downloads/Data_Pre_processing_in_Spam_Detection.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                messages\n",
       "count   5572                    5572\n",
       "unique     2                    5169\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4825                      30"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f\"{_PATH}/{_FILE_NAME}\")\n",
    "df.columns = [_LABEL, _MSG]\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "_STEMMER = PorterStemmer()\n",
    "\n",
    "def text_preprocess(message: str, stemmer: object = _STEMMER) -> list:\n",
    "    tokens = word_tokenize(message)\n",
    "    final_list = []\n",
    "    for token in tokens:\n",
    "        out_punc = re.sub(r'\\W+', \"\", token)\n",
    "        if len(out_punc) > 2 and out_punc.lower() not in en_stop_words:\n",
    "            stem = stemmer.stem(out_punc)\n",
    "            final_list.append(stem)\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of spam messages: 747\n",
      "Number of ham messages: 4825\n"
     ]
    }
   ],
   "source": [
    "spam_messages = df[df[_LABEL] == \"spam\"][_MSG]\n",
    "ham_messages = df[df[_LABEL] == \"ham\"][_MSG]\n",
    "print(f\"Number of spam messages: {len(spam_messages)}\")\n",
    "print(f\"Number of ham messages: {len(ham_messages)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 spam words are:\n",
      "call     366\n",
      "free     219\n",
      "txt      169\n",
      "text     139\n",
      "mobil    135\n",
      "stop     118\n",
      "claim    115\n",
      "repli    112\n",
      "prize     94\n",
      "get       88\n",
      "dtype: int64\n",
      "---------------------------------\n",
      "Top 10 ham words are:\n",
      "        1220\n",
      "get      360\n",
      "nt       351\n",
      "come     298\n",
      "call     294\n",
      "know     249\n",
      "love     247\n",
      "like     246\n",
      "got      245\n",
      "good     232\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def _top_10_word_by_label(messages: list, label: str) -> None:\n",
    "    words = []\n",
    "    for message in messages:\n",
    "        words += text_preprocess(message)\n",
    "    print(f\"Top 10 {label} words are:\\n{pd.Series(words).value_counts().head(10)}\")\n",
    "\n",
    "_top_10_word_by_label(spam_messages, \"spam\")\n",
    "print(\"---------------------------------\")\n",
    "_top_10_word_by_label(ham_messages, \"ham\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>messages</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>jurong point crazi avail bugi great world buff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>lar  joke wif oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entri wkli comp win cup final tkt 21st ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>dun say earli hor  alreadi say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah nt think goe usf live around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                           messages  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                               token  \n",
       "0  jurong point crazi avail bugi great world buff...  \n",
       "1                                 lar  joke wif oni   \n",
       "2  free entri wkli comp win cup final tkt 21st ma...  \n",
       "3                    dun say earli hor  alreadi say   \n",
       "4            nah nt think goe usf live around though  "
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[_TOKEN] = df[_MSG].apply(text_preprocess)\n",
    "df[_TOKEN] = df[_TOKEN].agg(lambda x: \" \".join(map(str, x)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 BOW Features: ['07090201529', '07090298926', '07099833605', '071104', '07123456789', '0721072', '07732584351', '07734396839', '07742676969', '07753741225', '0776xxxxxxx', '07786200117', '077xxx', '078', '07801543489', '07808', '07808247860', '07808726822', '07815296484', '07821230901']\n",
      "Total number of vocab words: 7545\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words Method\n",
    "vectorizer = CountVectorizer()\n",
    "bow_transformer = vectorizer.fit(df[_TOKEN])\n",
    "\n",
    "# Fetch the vocabulary set\n",
    "print(f\"20 BOW Features: {vectorizer.get_feature_names()[20:40]}\")\n",
    "print(f\"Total number of vocab words: {len(vectorizer.vocabulary_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of sparse matrix: (5572, 7545)\n",
      "Amount of non-zero occurrences: 43759\n"
     ]
    }
   ],
   "source": [
    "# Convert strings to vectors using BoW\n",
    "messages_bow = bow_transformer.transform(df[_TOKEN])\n",
    "print(f\"Shape of sparse matrix: {messages_bow.shape}\")\n",
    "print(f\"Amount of non-zero occurrences: {messages_bow.nnz}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 7545)\n"
     ]
    }
   ],
   "source": [
    "# Transform entire BoW into tf-idf corpus\n",
    "tfidf_transformer = TfidfTransformer().fit(messages_bow)\n",
    "messages_tfidf = tfidf_transformer.transform(messages_bow)\n",
    "print(messages_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>messages</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>jurong point crazi avail bugi great world buff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>lar  joke wif oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entri wkli comp win cup final tkt 21st ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>dun say earli hor  alreadi say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah nt think goe usf live around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                           messages  \\\n",
       "0      0  Go until jurong point, crazy.. Available only ...   \n",
       "1      0                      Ok lar... Joking wif u oni...   \n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3      0  U dun say so early hor... U c already then say...   \n",
       "4      0  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                               token  \n",
       "0  jurong point crazi avail bugi great world buff...  \n",
       "1                                 lar  joke wif oni   \n",
       "2  free entri wkli comp win cup final tkt 21st ma...  \n",
       "3                    dun say earli hor  alreadi say   \n",
       "4            nah nt think goe usf live around though  "
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert ham and spam labels to 0 and 1\n",
    "FactorResult = pd.factorize(df[_LABEL])\n",
    "df[_LABEL] = FactorResult[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset features size: (3900, 7545)\n",
      "Train dataset label size: (3900,)\n",
      "\n",
      "Test dataset features size: (1672, 7545)\n",
      "Test dataset label size: (1672,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def _print_shape(X: object, y: object, set: str) -> None:\n",
    "    print(f\"{set} dataset features size: {X.shape}\")\n",
    "    print(f\"{set} dataset label size: {y.shape}\\n\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    messages_tfidf, df[_LABEL], test_size=0.3, stratify=df[_LABEL].tolist()\n",
    ")\n",
    "\n",
    "_print_shape(X_train, y_train, \"Train\")\n",
    "_print_shape(X_test, y_test, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for the test dataset: 97.8%\n",
      "Confusion Matrix:\n",
      "[[1438   27]\n",
      " [  10  197]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      1448\n",
      "           1       0.95      0.88      0.91       224\n",
      "\n",
      "    accuracy                           0.98      1672\n",
      "   macro avg       0.97      0.94      0.95      1672\n",
      "weighted avg       0.98      0.98      0.98      1672\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def _get_accuracy(predict: object, y: object, dataset: str):\n",
    "    print(f\"Accuracy for the {dataset} dataset: {accuracy_score(y, predict)*100:.3}%\")\n",
    "    print(f\"Confusion Matrix:\\n{confusion_matrix(predict, y)}\")\n",
    "    print(classification_report(y, predict))\n",
    "    print()\n",
    "\n",
    "#_get_accuracy(predict_train, y_train, \"train\")\n",
    "_get_accuracy(predict_test, y_test, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Train:  0.9682051282051282\n",
      "Eval Test :  0.958730896416123\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "k_folds = sklearn.model_selection.KFold(n_splits = 5)\n",
    "values = sklearn.model_selection.cross_val_score(clf, X_train, y_train, cv = k_folds, scoring=\"accuracy\")\n",
    "print(\"Eval Train: \", values.mean())\n",
    "values = sklearn.model_selection.cross_val_score(clf, X_test, y_test, cv = k_folds, scoring=\"accuracy\")\n",
    "print(\"Eval Test : \", values.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _filter_punctuaction(tokens: str) -> None:\n",
    "    tokens_list = []\n",
    "    for token in tokens:\n",
    "        token_out_punctuation = re.sub(r'\\W+', \"\", token)\n",
    "        if len(token_out_punctuation) > 1:\n",
    "            tokens_list.append(token_out_punctuation)\n",
    "    return tokens_list\n",
    "\n",
    "def _get_tokens(message: str) -> list:\n",
    "    all_tokens = word_tokenize(message)\n",
    "    filtered_tokens = [word for word in all_tokens if not word.lower() in en_stop_words]\n",
    "    tokens_list = _filter_punctuaction(filtered_tokens)\n",
    "    return tokens_list\n",
    "\n",
    "df[_TOKEN] = df[_MSG].apply(_get_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[_LENGTH] = df[_MSG].apply(len)\n",
    "df.sort_values(by=_LENGTH, ascending=False).head(10)\n",
    "df.hist(column=_LENGTH, by=_LABEL,figsize=(12,4), bins = 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "a must be greater than 0 unless no samples are taken",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-313-6576e68b4a20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# one way to fix it is to downsample the ham msg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mham_msg_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_LABEL\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ham\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspam_messages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m44\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mspam_msg_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_LABEL\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"spam\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mham_msg_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspam_msg_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mham_msg_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspam_msg_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2021.05/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis)\u001b[0m\n\u001b[1;32m   5348\u001b[0m             )\n\u001b[1;32m   5349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5350\u001b[0;31m         \u001b[0mlocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5351\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: a must be greater than 0 unless no samples are taken"
     ]
    }
   ],
   "source": [
    "# one way to fix it is to downsample the ham msg\n",
    "ham_msg_df = df[df[_LABEL] == \"ham\"].sample(n=len(spam_messages), random_state = 44)\n",
    "spam_msg_df = df[df[_LABEL] == \"spam\"]\n",
    "print(ham_msg_df.shape, spam_msg_df.shape)\n",
    "df = pd.DataFrame(ham_msg_df.append(spam_msg_df).reset_index(drop=True))\n",
    "df.columns = [_LABEL, _MSG]\n",
    "print(len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Remove punctuations\n",
    "no_punc = [char for char in message if char not in string.punctuation]\n",
    "\n",
    "# Join the characters again\n",
    "no_punc = \"\".join(no_punc)\n",
    "no_punc = no_punc.lower()\n",
    "\n",
    "# Remove any stopwords and non-alphabetic characters\n",
    "no_stop_word = [\n",
    "    word\n",
    "    for word in no_punc.split()\n",
    "    if word.lower() not in en_stop_words and word.isalpha()\n",
    "]\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8 (default, Apr 13 2021, 12:59:45) \n[Clang 10.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "624c434b4448266ef6b3c5b6c15667705777b54e221aadd916740c58273314d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
